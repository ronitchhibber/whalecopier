"""
Final method: Direct HTML scraping + public data aggregation.
Combines multiple sources to reach 1000 whales.
"""

import os
import sys
import requests
import re
from datetime import datetime
from sqlalchemy import create_engine
from sqlalchemy.orm import Session

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from libs.common.models import Whale
from dotenv import load_dotenv

load_dotenv()

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://trader:changeme123@localhost:5432/polymarket_trader')
engine = create_engine(DATABASE_URL)


def scrape_github_whale_lists():
    """Scrape publicly shared whale lists from GitHub."""
    print("\n" + "="*80)
    print("METHOD: GitHub Public Whale Lists")
    print("="*80)

    addresses = set()

    # Search GitHub for Polymarket whale lists
    github_searches = [
        "https://raw.githubusercontent.com/search?q=polymarket+whale+addresses",
        "https://api.github.com/search/code?q=polymarket+0x+in:file",
    ]

    # Known public repos with Polymarket data
    known_repos = [
        "https://raw.githubusercontent.com/Polymarket/data/main/whales.json",
        "https://raw.githubusercontent.com/Polymarket/leaderboard/main/addresses.txt",
    ]

    for url in known_repos:
        try:
            print(f"ğŸ” Checking: {url}")
            response = requests.get(url, timeout=10)

            if response.status_code == 200:
                content = response.text

                # Extract all Ethereum addresses
                eth_addresses = re.findall(r'0x[a-fA-F0-9]{40}', content)

                for addr in eth_addresses:
                    addresses.add(addr.lower())

                print(f"   âœ… Found {len(eth_addresses)} addresses")
        except:
            pass

    print(f"âœ… Total from GitHub: {len(addresses)}")
    return list(addresses)


def scrape_etherscan_token_holders():
    """Get top holders of Polymarket-related tokens."""
    print("\n" + "="*80)
    print("METHOD: Token Holders Analysis")
    print("="*80)

    addresses = set()

    # UMA token address (used by Polymarket)
    # CTF token addresses
    token_addresses = [
        "0x59325733eb952a92e069C87F0A6168b29E80627f",  # Conditional Tokens Framework
    ]

    print("âš ï¸  This method requires Etherscan/Polygonscan API")
    print("   Placeholder - implement when API available")

    print(f"âœ… Total from tokens: {len(addresses)}")
    return list(addresses)


def extract_from_social_media_archives():
    """Extract addresses from archived social media posts."""
    print("\n" + "="*80)
    print("METHOD: Social Media Archives")
    print("="*80)

    addresses = set()

    # Twitter archive searches (public archives)
    twitter_archives = [
        "https://web.archive.org/web/*/polymarket.com/profile/*",
    ]

    print("âš ï¸  This requires web scraping archives")
    print("   Use manual collection for now")

    print(f"âœ… Total from archives: {len(addresses)}")
    return list(addresses)


def generate_sample_whale_dataset():
    """Generate a sample dataset of whale addresses for testing."""
    print("\n" + "="*80)
    print("METHOD: Sample Whale Dataset (For Testing)")
    print("="*80)

    # These are EXAMPLE addresses - you should replace with real ones
    # from https://polymarket.com/leaderboard
    sample_whales = [
        # Format: (address, pseudonym, approx_volume)
        # Add real addresses from leaderboard here
    ]

    addresses = [w[0] for w in sample_whales]

    print(f"âœ… Sample dataset: {len(addresses)} addresses")
    print("\nğŸ’¡ TIP: Visit https://polymarket.com/leaderboard")
    print("   Copy addresses from top 100 traders manually")

    return addresses


def bulk_import_from_csv():
    """Import whales from a CSV file if it exists."""
    print("\n" + "="*80)
    print("METHOD: CSV Bulk Import")
    print("="*80)

    addresses = []
    csv_path = "whale_addresses.csv"

    if os.path.exists(csv_path):
        print(f"ğŸ“„ Found {csv_path}")
        import csv

        with open(csv_path, 'r') as f:
            reader = csv.reader(f)
            next(reader, None)  # Skip header

            for row in reader:
                if row and row[0].startswith('0x'):
                    addresses.append(row[0].lower())

        print(f"âœ… Imported {len(addresses)} addresses from CSV")
    else:
        print("âš ï¸  No CSV file found")
        print(f"   Create {csv_path} with format:")
        print("   address,pseudonym")
        print("   0x1234...,TraderName")

    return addresses


def add_whales_to_db(addresses):
    """Add addresses to database."""
    if not addresses:
        return 0

    print("\n" + "="*80)
    print(f"ğŸ’¾ ADDING {len(addresses)} WHALES")
    print("="*80)

    with Session(engine) as session:
        added = 0
        skipped = 0

        for address in addresses:
            try:
                existing = session.query(Whale).filter(Whale.address == address).first()

                if existing:
                    skipped += 1
                    continue

                from eth_utils import to_checksum_address
                try:
                    checksummed = to_checksum_address(address)
                except:
                    checksummed = address

                whale = Whale(
                    address=checksummed,
                    pseudonym=f"Whale_{checksummed[2:10]}",
                    tier="MEDIUM",
                    quality_score=50.0,
                    total_volume=0.0,
                    total_trades=0,
                    win_rate=0.0,
                    sharpe_ratio=0.0,
                    total_pnl=0.0,
                    is_copying_enabled=True,
                    last_active=datetime.utcnow()
                )

                session.add(whale)

                if (added + 1) % 100 == 0:
                    session.commit()
                    print(f"   âœ… Committed batch {(added + 1)//100}")

                added += 1

            except Exception as e:
                session.rollback()
                continue

        session.commit()

        print(f"\nâœ… Added: {added} | Skipped: {skipped}")
        return added


def main():
    print("\n" + "="*80)
    print("ğŸ” COMPREHENSIVE WHALE DISCOVERY")
    print("="*80)

    all_addresses = set()

    # Try all methods
    methods = [
        ("GitHub Lists", scrape_github_whale_lists),
        ("Token Holders", scrape_etherscan_token_holders),
        ("Social Archives", extract_from_social_media_archives),
        ("CSV Import", bulk_import_from_csv),
        ("Sample Dataset", generate_sample_whale_dataset),
    ]

    for name, method in methods:
        print(f"\nğŸ” Trying: {name}")
        try:
            addresses = method()
            all_addresses.update(addresses)
            print(f"   Running total: {len(all_addresses)} addresses")
        except Exception as e:
            print(f"   âŒ Failed: {e}")

    # Filter
    filtered = [addr for addr in all_addresses if addr.startswith('0x') and len(addr) == 42]

    print(f"\n" + "="*80)
    print(f"ğŸ“Š FINAL RESULTS")
    print("="*80)
    print(f"Total addresses: {len(filtered)}")

    if filtered:
        added = add_whales_to_db(filtered)

        # Check total in DB
        with Session(engine) as session:
            total_count = session.query(Whale).count()

        print(f"\nğŸ‰ Database now has {total_count} total whales")

        if total_count >= 1000:
            print("ğŸ¯ TARGET REACHED: 1000+ whales!")
        else:
            print(f"ğŸ“ˆ Progress: {total_count}/1000 whales")
            print("\nTo reach 1000 whales:")
            print("1. Create whale_addresses.csv with addresses from leaderboard")
            print("2. Run this script again")
            print("3. Or use: python3 scripts/add_whale_address.py <ADDRESS>")
    else:
        print("\nâš ï¸  No new addresses found")
        print("\nğŸ’¡ RECOMMENDED: Manual Collection")
        print("   1. Visit: https://polymarket.com/leaderboard")
        print("   2. For each top trader:")
        print("      - Click username")
        print("      - Copy address from URL")
        print("      - Run: python3 scripts/add_whale_address.py <ADDRESS>")
        print("   3. Or create whale_addresses.csv and import in bulk")


if __name__ == "__main__":
    main()
